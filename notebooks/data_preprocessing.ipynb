{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NSL-KDD Data Preprocessing for Cybersecurity Threat Detection\n",
    "\n",
    "This notebook preprocesses the NSL-KDD dataset for training an XGBoost model on AWS SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sagemaker\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SageMaker session\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Define bucket names (replace with your actual bucket names)\n",
    "raw_bucket = 'cybersec-raw-data-xxxxxxxx'  # Replace with actual bucket name\n",
    "processed_bucket = 'cybersec-processed-data-xxxxxxxx'  # Replace with actual bucket name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names for NSL-KDD dataset\n",
    "columns = [\n",
    "    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes',\n",
    "    'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in',\n",
    "    'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations',\n",
    "    'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login',\n",
    "    'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate',\n",
    "    'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate',\n",
    "    'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count',\n",
    "    'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',\n",
    "    'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate',\n",
    "    'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'label', 'difficulty'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load NSL-KDD data\n",
    "# You need to upload the NSL-KDD files to your raw data bucket first\n",
    "\n",
    "# Load training data\n",
    "train_data = pd.read_csv('s3://' + raw_bucket + '/KDDTrain+.txt', names=columns)\n",
    "test_data = pd.read_csv('s3://' + raw_bucket + '/KDDTest+.txt', names=columns)\n",
    "\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Test data shape: {test_data.shape}\")\n",
    "print(f\"\\nLabel distribution in training data:\")\n",
    "print(train_data['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"Preprocess the NSL-KDD dataset\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Remove difficulty column\n",
    "    if 'difficulty' in df.columns:\n",
    "        df = df.drop('difficulty', axis=1)\n",
    "    \n",
    "    # Convert label to binary (0: normal, 1: attack)\n",
    "    df['label'] = df['label'].apply(lambda x: 0 if x == 'normal' else 1)\n",
    "    \n",
    "    # Encode categorical features\n",
    "    categorical_features = ['protocol_type', 'service', 'flag']\n",
    "    \n",
    "    for feature in categorical_features:\n",
    "        le = LabelEncoder()\n",
    "        df[feature] = le.fit_transform(df[feature])\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = df.drop('label', axis=1)\n",
    "    y = df['label']\n",
    "    \n",
    "    # Normalize continuous features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "    \n",
    "    return X_scaled, y, scaler\n",
    "\n",
    "# Preprocess training and test data\n",
    "X_train, y_train, scaler = preprocess_data(train_data)\n",
    "X_test, y_test, _ = preprocess_data(test_data)\n",
    "\n",
    "print(f\"Preprocessed training data shape: {X_train.shape}\")\n",
    "print(f\"Training labels distribution: {y_train.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for XGBoost (target as first column)\n",
    "train_data_xgb = pd.concat([y_train, X_train], axis=1)\n",
    "test_data_xgb = pd.concat([y_test, X_test], axis=1)\n",
    "\n",
    "# Save preprocessed data to S3\n",
    "train_data_xgb.to_csv(f's3://{processed_bucket}/train/train.csv', index=False, header=False)\n",
    "test_data_xgb.to_csv(f's3://{processed_bucket}/test/test.csv', index=False, header=False)\n",
    "\n",
    "# Save validation data (20% of training data)\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "val_data_xgb = pd.concat([y_val, X_val], axis=1)\n",
    "val_data_xgb.to_csv(f's3://{processed_bucket}/validation/validation.csv', index=False, header=False)\n",
    "\n",
    "print(\"Data preprocessing completed and saved to S3!\")\n",
    "print(f\"Training data: s3://{processed_bucket}/train/train.csv\")\n",
    "print(f\"Validation data: s3://{processed_bucket}/validation/validation.csv\")\n",
    "print(f\"Test data: s3://{processed_bucket}/test/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature names for later use\n",
    "feature_names = X_train.columns.tolist()\n",
    "feature_info = {\n",
    "    'feature_names': feature_names,\n",
    "    'num_features': len(feature_names)\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('/tmp/feature_info.json', 'w') as f:\n",
    "    json.dump(feature_info, f)\n",
    "\n",
    "# Upload feature info to S3\n",
    "s3.upload_file('/tmp/feature_info.json', processed_bucket, 'feature_info.json')\n",
    "print(\"Feature information saved to S3\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}