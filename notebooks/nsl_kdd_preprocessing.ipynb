{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NSL-KDD Threat Detection - Complete Data Pipeline\n",
    "## Data Preprocessing, Model Training & Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = ['pandas', 'numpy', 'scikit-learn', 'sagemaker', 'boto3']\n",
    "for package in packages:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", package])\n",
    "\n",
    "print(\"‚úÖ Packages installed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.inputs import TrainingInput\n",
    "import os\n",
    "import urllib.request\n",
    "import json\n",
    "\n",
    "# Initialize SageMaker session\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "print(f\"SageMaker role: {role}\")\n",
    "print(f\"Region: {region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get S3 bucket names from environment or use defaults\n",
    "raw_bucket = 'cybersec-raw-data-plh92c1q'\n",
    "processed_bucket = 'cybersec-processed-data-plh92c1q'\n",
    "\n",
    "print(f\"Raw data bucket: {raw_bucket}\")\n",
    "print(f\"Processed data bucket: {processed_bucket}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NSL-KDD column names (41 features + label)\n",
    "columns = [\n",
    "    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes',\n",
    "    'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in',\n",
    "    'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations',\n",
    "    'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login',\n",
    "    'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate',\n",
    "    'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate',\n",
    "    'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count',\n",
    "    'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',\n",
    "    'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate',\n",
    "    'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'label', 'difficulty'\n",
    "]\n",
    "\n",
    "print(f\"Total columns: {len(columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NSL-KDD dataset\n",
    "print(\"üì• Downloading NSL-KDD dataset...\")\n",
    "\n",
    "train_url = \"https://raw.githubusercontent.com/defcom17/NSL_KDD/master/KDDTrain%2B.txt\"\n",
    "test_url = \"https://raw.githubusercontent.com/defcom17/NSL_KDD/master/KDDTest%2B.txt\"\n",
    "\n",
    "urllib.request.urlretrieve(train_url, '/tmp/KDDTrain+.txt')\n",
    "urllib.request.urlretrieve(test_url, '/tmp/KDDTest+.txt')\n",
    "\n",
    "print(\"‚úÖ Dataset downloaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and examine the data\n",
    "print(\"üìä Loading and examining data...\")\n",
    "\n",
    "train_df = pd.read_csv('/tmp/KDDTrain+.txt', names=columns, header=None)\n",
    "test_df = pd.read_csv('/tmp/KDDTest+.txt', names=columns, header=None)\n",
    "\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(f\"\\nUnique labels in training data: {train_df['label'].unique()}\")\n",
    "print(f\"Label distribution:\\n{train_df['label'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing Function\n",
    "def preprocess_nsl_kdd(train_df, test_df):\n",
    "    print(\"üîß Starting data preprocessing...\")\n",
    "    \n",
    "    # Combine datasets for consistent encoding\n",
    "    combined_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "    \n",
    "    # 1. Convert labels to binary (0=normal, 1=attack)\n",
    "    print(\"Converting labels to binary classification...\")\n",
    "    combined_df['label_binary'] = (combined_df['label'] != 'normal').astype(int)\n",
    "    \n",
    "    # 2. Encode categorical features\n",
    "    print(\"Encoding categorical features...\")\n",
    "    categorical_features = ['protocol_type', 'service', 'flag']\n",
    "    label_encoders = {}\n",
    "    \n",
    "    for feature in categorical_features:\n",
    "        le = LabelEncoder()\n",
    "        combined_df[feature] = le.fit_transform(combined_df[feature])\n",
    "        label_encoders[feature] = le\n",
    "        print(f\"  - {feature}: {len(le.classes_)} unique values\")\n",
    "    \n",
    "    # 3. Identify continuous features (exclude categorical and target)\n",
    "    continuous_features = [col for col in combined_df.columns \n",
    "                          if col not in categorical_features + ['label', 'label_binary', 'difficulty']]\n",
    "    \n",
    "    print(f\"Continuous features: {len(continuous_features)}\")\n",
    "    \n",
    "    # 4. Normalize continuous features using StandardScaler\n",
    "    print(\"Normalizing continuous features...\")\n",
    "    scaler = StandardScaler()\n",
    "    combined_df[continuous_features] = scaler.fit_transform(combined_df[continuous_features])\n",
    "    \n",
    "    # 5. Drop unnecessary columns\n",
    "    combined_df = combined_df.drop(['label', 'difficulty'], axis=1)\n",
    "    \n",
    "    # 6. Reorder columns (label first for XGBoost)\n",
    "    feature_cols = [col for col in combined_df.columns if col != 'label_binary']\n",
    "    combined_df = combined_df[['label_binary'] + feature_cols]\n",
    "    \n",
    "    # 7. Split back into train and test\n",
    "    train_processed = combined_df[:len(train_df)].copy()\n",
    "    test_processed = combined_df[len(train_df):].copy()\n",
    "    \n",
    "    print(f\"‚úÖ Preprocessing completed!\")\n",
    "    print(f\"Final training shape: {train_processed.shape}\")\n",
    "    print(f\"Final test shape: {test_processed.shape}\")\n",
    "    print(f\"Binary label distribution: {train_processed['label_binary'].value_counts().to_dict()}\")\n",
    "    \n",
    "    return train_processed, test_processed, label_encoders, scaler\n",
    "\n",
    "# Execute preprocessing\n",
    "train_processed, test_processed, encoders, scaler = preprocess_nsl_kdd(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/validation split\n",
    "print(\"üìä Creating train/validation split...\")\n",
    "\n",
    "# Split training data into train/validation (80/20)\n",
    "train_data, val_data = train_test_split(train_processed, test_size=0.2, random_state=42, \n",
    "                                       stratify=train_processed['label_binary'])\n",
    "\n",
    "print(f\"Training set: {train_data.shape}\")\n",
    "print(f\"Validation set: {val_data.shape}\")\n",
    "print(f\"Test set: {test_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data to S3\n",
    "print(\"üíæ Saving processed data to S3...\")\n",
    "\n",
    "# Save locally first\n",
    "train_data.to_csv('/tmp/train_processed.csv', index=False, header=False)\n",
    "val_data.to_csv('/tmp/validation_processed.csv', index=False, header=False)\n",
    "test_processed.to_csv('/tmp/test_processed.csv', index=False, header=False)\n",
    "\n",
    "# Upload to S3\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "s3.upload_file('/tmp/train_processed.csv', processed_bucket, 'train/train.csv')\n",
    "s3.upload_file('/tmp/validation_processed.csv', processed_bucket, 'validation/validation.csv')\n",
    "s3.upload_file('/tmp/test_processed.csv', processed_bucket, 'test/test.csv')\n",
    "\n",
    "# Save preprocessing artifacts\n",
    "import pickle\n",
    "\n",
    "with open('/tmp/encoders.pkl', 'wb') as f:\n",
    "    pickle.dump(encoders, f)\n",
    "    \n",
    "with open('/tmp/scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "s3.upload_file('/tmp/encoders.pkl', processed_bucket, 'artifacts/encoders.pkl')\n",
    "s3.upload_file('/tmp/scaler.pkl', processed_bucket, 'artifacts/scaler.pkl')\n",
    "\n",
    "print(\"‚úÖ Data saved to S3 successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure XGBoost training\n",
    "print(\"üöÄ Configuring XGBoost model training...\")\n",
    "\n",
    "# Define S3 paths\n",
    "train_path = f's3://{processed_bucket}/train/'\n",
    "validation_path = f's3://{processed_bucket}/validation/'\n",
    "output_path = f's3://{processed_bucket}/model-output/'\n",
    "\n",
    "# Get XGBoost container\n",
    "container = sagemaker.image_uris.retrieve('xgboost', region, version='1.5-1')\n",
    "\n",
    "# Create XGBoost estimator\n",
    "xgb_estimator = Estimator(\n",
    "    image_uri=container,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    output_path=output_path,\n",
    "    sagemaker_session=sess,\n",
    "    hyperparameters={\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'num_round': 100,\n",
    "        'max_depth': 6,\n",
    "        'eta': 0.1,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'min_child_weight': 3,\n",
    "        'gamma': 0.1,\n",
    "        'reg_alpha': 0.1,\n",
    "        'reg_lambda': 1,\n",
    "        'scale_pos_weight': 1,\n",
    "        'early_stopping_rounds': 10,\n",
    "        'verbosity': 1\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"‚úÖ XGBoost estimator configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"üîÑ Starting model training...\")\n",
    "\n",
    "train_input = TrainingInput(train_path, content_type='text/csv')\n",
    "validation_input = TrainingInput(validation_path, content_type='text/csv')\n",
    "\n",
    "xgb_estimator.fit({\n",
    "    'train': train_input,\n",
    "    'validation': validation_input\n",
    "})\n",
    "\n",
    "print(\"‚úÖ Model training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy model to endpoint\n",
    "print(\"üöÄ Deploying model to endpoint...\")\n",
    "\n",
    "import time\n",
    "endpoint_name = f'threat-detection-endpoint-{int(time.time())}'\n",
    "\n",
    "predictor = xgb_estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.t2.medium',\n",
    "    endpoint_name=endpoint_name\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model deployed to endpoint: {predictor.endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the endpoint with sample data\n",
    "print(\"üß™ Testing the endpoint...\")\n",
    "\n",
    "# Get a sample from test data (excluding label)\n",
    "test_sample = test_processed.iloc[0, 1:].values  # Exclude label column\n",
    "csv_input = ','.join(map(str, test_sample))\n",
    "\n",
    "# Make prediction\n",
    "result = predictor.predict(csv_input)\n",
    "prediction = float(result)\n",
    "\n",
    "print(f\"Sample input shape: {len(test_sample)}\")\n",
    "print(f\"Raw prediction: {prediction}\")\n",
    "print(f\"Binary prediction: {1 if prediction > 0.5 else 0}\")\n",
    "print(f\"Confidence: {prediction if prediction > 0.5 else 1-prediction:.4f}\")\n",
    "print(f\"Status: {'Attack Detected' if prediction > 0.5 else 'Normal Traffic'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save endpoint information and update Lambda\n",
    "print(\"üíæ Saving endpoint information...\")\n",
    "\n",
    "endpoint_info = {\n",
    "    'endpoint_name': predictor.endpoint_name,\n",
    "    'instance_type': 'ml.t2.medium',\n",
    "    'status': 'InService',\n",
    "    'model_features': len(test_sample),\n",
    "    'created_at': time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "}\n",
    "\n",
    "# Save to S3\n",
    "s3.put_object(\n",
    "    Bucket=processed_bucket,\n",
    "    Key='endpoint_info.json',\n",
    "    Body=json.dumps(endpoint_info, indent=2)\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Endpoint info saved: {endpoint_info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Lambda function environment variable\n",
    "print(\"üîß Updating Lambda function...\")\n",
    "\n",
    "try:\n",
    "    lambda_client = boto3.client('lambda')\n",
    "    \n",
    "    # Find Lambda function\n",
    "    functions = lambda_client.list_functions()['Functions']\n",
    "    lambda_function = None\n",
    "    \n",
    "    for func in functions:\n",
    "        if 'threat-detection-predict' in func['FunctionName']:\n",
    "            lambda_function = func['FunctionName']\n",
    "            break\n",
    "    \n",
    "    if lambda_function:\n",
    "        # Update environment variable\n",
    "        lambda_client.update_function_configuration(\n",
    "            FunctionName=lambda_function,\n",
    "            Environment={\n",
    "                'Variables': {\n",
    "                    'ENDPOINT_NAME': predictor.endpoint_name\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        print(f\"‚úÖ Lambda function {lambda_function} updated with endpoint: {predictor.endpoint_name}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Lambda function not found\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not update Lambda: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final validation and summary\n",
    "print(\"\\nüéâ DEPLOYMENT SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"‚úÖ Dataset: NSL-KDD processed successfully\")\n",
    "print(f\"‚úÖ Training samples: {len(train_data):,}\")\n",
    "print(f\"‚úÖ Validation samples: {len(val_data):,}\")\n",
    "print(f\"‚úÖ Test samples: {len(test_processed):,}\")\n",
    "print(f\"‚úÖ Features: {len(test_sample)} (normalized)\")\n",
    "print(f\"‚úÖ Model: XGBoost binary classifier\")\n",
    "print(f\"‚úÖ Endpoint: {predictor.endpoint_name}\")\n",
    "print(f\"‚úÖ Status: Ready for predictions\")\n",
    "print(\"\\nüåê Frontend can now make real predictions!\")\n",
    "print(f\"üìä S3 Buckets:\")\n",
    "print(f\"   - Raw data: s3://{raw_bucket}\")\n",
    "print(f\"   - Processed data: s3://{processed_bucket}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}